{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5a20732e-6c90-418a-9817-4c1b4e2a2077",
      "metadata": {
        "tags": [],
        "id": "5a20732e-6c90-418a-9817-4c1b4e2a2077"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.profiler import profile, record_function, ProfilerActivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0f92994f-ffe8-4e80-9c46-8963aade1246",
      "metadata": {
        "tags": [],
        "id": "0f92994f-ffe8-4e80-9c46-8963aade1246"
      },
      "outputs": [],
      "source": [
        "def build_data_loader(data_dir, batch_size, random_seed=42, valid_size=0.1, shuffle=True, test=False):\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    \n",
        "    train_dataset = datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
        "    valid_dataset = datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
        "  \n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return (train_loader, valid_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d745f184-7704-4850-9f57-4268cf20102e",
      "metadata": {
        "tags": [],
        "id": "d745f184-7704-4850-9f57-4268cf20102e"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, valid_loader, num_epochs, criterion, optimizer, device):\n",
        "    total_steps = len(train_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, (images, labels) in enumerate(train_loader):  \n",
        "            # Move tensors to the configured device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "        \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        \n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print ('Step [{}/{}], Loss: {:.4f}'.format(step+1, total_steps, loss.item()))\n",
        "               \n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e811ffaf-11fb-4a57-b16e-595338a32081",
      "metadata": {
        "tags": [],
        "id": "e811ffaf-11fb-4a57-b16e-595338a32081"
      },
      "outputs": [],
      "source": [
        "def validate(model, valid_loader, device):\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "    \n",
        "    print('Accuracy of the network on the {} validation images: {:.2f} %'.format(5000, 100 * correct / total)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e1843b38-0eff-476b-8ed0-2a1646dbb72f",
      "metadata": {
        "tags": [],
        "id": "e1843b38-0eff-476b-8ed0-2a1646dbb72f"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, device):\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce883513-47bf-45b0-b902-807dc848c733",
      "metadata": {
        "tags": [],
        "id": "ce883513-47bf-45b0-b902-807dc848c733"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    parameters = list(model.parameters())\n",
        "    total_parms = sum([np.prod(p.size()) for p in parameters if p.requires_grad])\n",
        "    return total_parms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        \n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 512),\n",
        "            nn.Dropout(0.25))\n",
        "        \n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "d_gHdGv84ocR"
      },
      "id": "d_gHdGv84ocR",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "76056f95-3978-44e4-94a0-a9c4c7ee21fc",
      "metadata": {
        "tags": [],
        "id": "76056f95-3978-44e4-94a0-a9c4c7ee21fc"
      },
      "outputs": [],
      "source": [
        "# General parameters\n",
        "data_dir = '/tmp'\n",
        "device = 'cpu'\n",
        "num_classes = 10\n",
        "\n",
        "# Hyperparameters\n",
        "max_lr = 0.00001\n",
        "weight_decay = 0.005\n",
        "batch_size = 64\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam\n",
        "\n",
        "# FashionMNIST dataset \n",
        "train_loader, valid_loader, test_loader = build_data_loader(data_dir=data_dir, batch_size=batch_size)\n",
        "\n",
        "# Model definition\n",
        "model = CNN()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optimizer(model.parameters(), max_lr, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a2d947-887b-4d3d-9ac0-bb6321f206d9",
      "metadata": {
        "tags": [],
        "id": "c3a2d947-887b-4d3d-9ac0-bb6321f206d9",
        "outputId": "87219b72-d275-482d-f805-3ab08da2205c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "138357544\n"
          ]
        }
      ],
      "source": [
        "print(count_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1b0241-6c87-40cf-8e55-d8cddf5cc71e",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db1b0241-6c87-40cf-8e55-d8cddf5cc71e",
        "outputId": "4e39020b-7a35-4bf0-e2c4-d862cb3257f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [1/844], Loss: 2.4562\n",
            "Step [2/844], Loss: 2.3195\n",
            "Step [3/844], Loss: 2.3232\n",
            "Step [4/844], Loss: 2.3027\n",
            "Step [5/844], Loss: 2.2284\n",
            "Step [6/844], Loss: 2.3011\n",
            "Step [7/844], Loss: 2.2842\n",
            "Step [8/844], Loss: 2.2664\n",
            "Step [9/844], Loss: 2.2091\n",
            "Step [10/844], Loss: 2.1720\n",
            "Step [11/844], Loss: 2.1101\n",
            "Step [12/844], Loss: 2.1202\n",
            "Step [13/844], Loss: 2.0983\n",
            "Step [14/844], Loss: 2.0700\n",
            "Step [15/844], Loss: 2.0615\n",
            "Step [16/844], Loss: 1.9833\n",
            "Step [17/844], Loss: 2.0228\n",
            "Step [18/844], Loss: 1.9637\n",
            "Step [19/844], Loss: 1.9869\n",
            "Step [20/844], Loss: 1.9376\n",
            "Step [21/844], Loss: 1.9018\n",
            "Step [22/844], Loss: 1.9081\n",
            "Step [23/844], Loss: 1.8654\n",
            "Step [24/844], Loss: 1.8457\n",
            "Step [25/844], Loss: 1.8280\n",
            "Step [26/844], Loss: 1.8223\n",
            "Step [27/844], Loss: 1.7437\n",
            "Step [28/844], Loss: 1.7903\n",
            "Step [29/844], Loss: 1.7964\n",
            "Step [30/844], Loss: 1.7667\n",
            "Step [31/844], Loss: 1.8466\n",
            "Step [32/844], Loss: 1.7255\n",
            "Step [33/844], Loss: 1.6633\n",
            "Step [34/844], Loss: 1.7211\n",
            "Step [35/844], Loss: 1.6883\n",
            "Step [36/844], Loss: 1.6112\n",
            "Step [37/844], Loss: 1.6010\n",
            "Step [38/844], Loss: 1.7020\n",
            "Step [39/844], Loss: 1.6757\n",
            "Step [40/844], Loss: 1.5468\n",
            "Step [41/844], Loss: 1.6197\n",
            "Step [42/844], Loss: 1.6726\n",
            "Step [43/844], Loss: 1.5535\n",
            "Step [44/844], Loss: 1.4698\n",
            "Step [45/844], Loss: 1.5817\n",
            "Step [46/844], Loss: 1.4683\n",
            "Step [47/844], Loss: 1.4180\n",
            "Step [48/844], Loss: 1.6167\n",
            "Step [49/844], Loss: 1.5806\n",
            "Step [50/844], Loss: 1.5097\n",
            "Step [51/844], Loss: 1.4640\n",
            "Step [52/844], Loss: 1.5162\n",
            "Step [53/844], Loss: 1.4502\n",
            "Step [54/844], Loss: 1.4893\n",
            "Step [55/844], Loss: 1.3706\n",
            "Step [56/844], Loss: 1.3252\n",
            "Step [57/844], Loss: 1.3762\n",
            "Step [58/844], Loss: 1.4521\n",
            "Step [59/844], Loss: 1.4090\n",
            "Step [60/844], Loss: 1.3843\n",
            "Step [61/844], Loss: 1.3993\n",
            "Step [62/844], Loss: 1.3612\n",
            "Step [63/844], Loss: 1.3883\n",
            "Step [64/844], Loss: 1.1980\n",
            "Step [65/844], Loss: 1.1913\n",
            "Step [66/844], Loss: 1.2999\n",
            "Step [67/844], Loss: 1.4310\n",
            "Step [68/844], Loss: 1.3216\n",
            "Step [69/844], Loss: 1.3881\n",
            "Step [70/844], Loss: 1.2667\n",
            "Step [71/844], Loss: 1.3584\n",
            "Step [72/844], Loss: 1.2270\n",
            "Step [73/844], Loss: 1.3693\n",
            "Step [74/844], Loss: 1.2656\n",
            "Step [75/844], Loss: 1.2091\n",
            "Step [76/844], Loss: 1.4058\n",
            "Step [77/844], Loss: 1.1802\n",
            "Step [78/844], Loss: 1.3202\n",
            "Step [79/844], Loss: 1.1575\n",
            "Step [80/844], Loss: 1.1869\n",
            "Step [81/844], Loss: 1.2486\n",
            "Step [82/844], Loss: 1.3971\n",
            "Step [83/844], Loss: 1.2793\n",
            "Step [84/844], Loss: 1.1612\n",
            "Step [85/844], Loss: 1.1963\n",
            "Step [86/844], Loss: 1.1876\n",
            "Step [87/844], Loss: 1.1583\n",
            "Step [88/844], Loss: 1.1569\n",
            "Step [89/844], Loss: 1.2990\n",
            "Step [90/844], Loss: 1.1206\n",
            "Step [91/844], Loss: 1.1808\n",
            "Step [92/844], Loss: 1.2673\n",
            "Step [93/844], Loss: 1.0907\n",
            "Step [94/844], Loss: 1.0975\n",
            "Step [95/844], Loss: 1.1456\n",
            "Step [96/844], Loss: 1.1252\n",
            "Step [97/844], Loss: 1.1865\n",
            "Step [98/844], Loss: 1.1475\n",
            "Step [99/844], Loss: 0.9916\n",
            "Step [100/844], Loss: 1.0636\n",
            "Step [101/844], Loss: 1.1290\n",
            "Step [102/844], Loss: 1.0662\n",
            "Step [103/844], Loss: 1.0915\n",
            "Step [104/844], Loss: 1.0568\n",
            "Step [105/844], Loss: 1.0806\n",
            "Step [106/844], Loss: 1.1532\n",
            "Step [107/844], Loss: 1.0050\n",
            "Step [108/844], Loss: 0.9513\n",
            "Step [109/844], Loss: 1.0285\n",
            "Step [110/844], Loss: 1.0968\n",
            "Step [111/844], Loss: 1.1064\n",
            "Step [112/844], Loss: 1.1275\n",
            "Step [113/844], Loss: 1.0524\n",
            "Step [114/844], Loss: 1.1387\n",
            "Step [115/844], Loss: 0.9568\n",
            "Step [116/844], Loss: 1.2404\n",
            "Step [117/844], Loss: 1.0173\n",
            "Step [118/844], Loss: 0.9598\n",
            "Step [119/844], Loss: 0.9908\n",
            "Step [120/844], Loss: 1.0274\n",
            "Step [121/844], Loss: 1.1254\n",
            "Step [122/844], Loss: 0.9726\n",
            "Step [123/844], Loss: 1.0361\n",
            "Step [124/844], Loss: 1.0131\n",
            "Step [125/844], Loss: 0.9976\n",
            "Step [126/844], Loss: 1.0348\n",
            "Step [127/844], Loss: 0.9711\n",
            "Step [128/844], Loss: 1.0280\n",
            "Step [129/844], Loss: 1.0042\n",
            "Step [130/844], Loss: 0.8549\n",
            "Step [131/844], Loss: 0.9642\n",
            "Step [132/844], Loss: 0.8930\n",
            "Step [133/844], Loss: 0.9546\n",
            "Step [134/844], Loss: 0.8412\n",
            "Step [135/844], Loss: 1.0837\n",
            "Step [136/844], Loss: 0.9490\n",
            "Step [137/844], Loss: 0.9435\n",
            "Step [138/844], Loss: 0.8827\n",
            "Step [139/844], Loss: 1.1164\n",
            "Step [140/844], Loss: 1.0272\n",
            "Step [141/844], Loss: 0.8737\n",
            "Step [142/844], Loss: 0.9882\n",
            "Step [143/844], Loss: 0.8177\n",
            "Step [144/844], Loss: 0.7913\n",
            "Step [145/844], Loss: 0.9395\n",
            "Step [146/844], Loss: 1.0764\n",
            "Step [147/844], Loss: 1.0988\n",
            "Step [148/844], Loss: 1.0192\n",
            "Step [149/844], Loss: 0.8699\n",
            "Step [150/844], Loss: 0.7742\n",
            "Step [151/844], Loss: 0.9292\n",
            "Step [152/844], Loss: 0.9512\n",
            "Step [153/844], Loss: 0.8972\n",
            "Step [154/844], Loss: 0.7957\n",
            "Step [155/844], Loss: 0.9887\n",
            "Step [156/844], Loss: 0.8339\n",
            "Step [157/844], Loss: 0.8880\n",
            "Step [158/844], Loss: 0.8203\n",
            "Step [159/844], Loss: 0.7986\n",
            "Step [160/844], Loss: 0.8290\n",
            "Step [161/844], Loss: 0.9228\n",
            "Step [162/844], Loss: 0.8252\n",
            "Step [163/844], Loss: 0.9107\n",
            "Step [164/844], Loss: 0.9120\n",
            "Step [165/844], Loss: 0.7375\n",
            "Step [166/844], Loss: 0.8716\n",
            "Step [167/844], Loss: 0.8333\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Train the model\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof:\n",
        "    with record_function(\"training\"):\n",
        "        train(model, train_loader, valid_loader, num_epochs, criterion, optimizer, device)\n",
        "        \n",
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e794d3a5-5955-4657-b191-b4b2c2b71b8b",
      "metadata": {
        "id": "e794d3a5-5955-4657-b191-b4b2c2b71b8b"
      },
      "outputs": [],
      "source": [
        "test(model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}